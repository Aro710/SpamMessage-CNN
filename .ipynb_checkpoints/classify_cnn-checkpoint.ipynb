{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(file):\n",
    "    x = []\n",
    "    y = []\n",
    "    try:\n",
    "        f = open(file)\n",
    "        fc = f.readlines()\n",
    "        print(len(fc))\n",
    "        for line in fc:\n",
    "            try:\n",
    "                y_t,x_t = line.strip().split('\\t')\n",
    "                x.append(x_t)\n",
    "                y.append(y_t)\n",
    "            except :\n",
    "                print(line)\n",
    "                print(line.strip().split('\\t'))\n",
    "                pass\n",
    "        return x,y\n",
    "    except IOError:\n",
    "        line = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def load_dict(dict_path):\n",
    "    char_dict_re = dict()\n",
    "    #dict_path = os.path.join(DATA_PATH, 'words.dict')\n",
    "    with open(dict_path, encoding='utf-8') as fin:\n",
    "        char_dict = json.load(fin)\n",
    "    for k, v in char_dict.items():\n",
    "        char_dict_re[v] = k\n",
    "    return char_dict, char_dict_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "def data_process(text_str):\n",
    "    if len(text_str) == 0:\n",
    "        print('[ERROR] data_process failed! | The params: {}'.format(text_str))\n",
    "        return None\n",
    "    text_str = text_str.strip().replace('\\s+', ' ', 3)\n",
    "    #jieba.lcut 返回中文分词的list\n",
    "    return jieba.lcut(text_str)\n",
    "\n",
    "# 每次处理一段话\n",
    "def word2id(text_str, word_dict, max_seq_len=128):\n",
    "    if len(text_str) == 0 or len(word_dict) == 0:\n",
    "        print('[ERROR] word2id failed! | The params: {} and {}'.format(text_str, word_dict))\n",
    "        return None\n",
    "\n",
    "    sent_list = data_process(text_str)\n",
    "    sent_ids = list()\n",
    "    for item in sent_list:\n",
    "        if item in word_dict:\n",
    "            sent_ids.append(word_dict[item])\n",
    "        else:\n",
    "            # unknown key 和pad\n",
    "            sent_ids.append(word_dict['_UNK_'])\n",
    "\n",
    "    if len(sent_ids) < max_seq_len:\n",
    "        sent_ids = sent_ids + [word_dict['_PAD_'] for _ in range(max_seq_len - len(sent_ids))]\n",
    "    else:\n",
    "        sent_ids = sent_ids[:max_seq_len]\n",
    "    return sent_ids\n",
    "\n",
    "\n",
    "def data2id(x,word_dict):\n",
    "    x = list(map(lambda t:word2id(t,word_dict),x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x,y = get_data('./data/带标签短信.txt')\n",
    "x= data2id(x,char_dict)\n",
    "X_train, X_test, y_train, y_test = train_test_split( np.array(x), np.array(y), test_size=0.3, random_state=42)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(x,size = 32):\n",
    "    data_size = len(x)\n",
    "    num = np.floor(data_size/size)\n",
    "    index = np.arange(data_size)\n",
    "    index = np.append(index,np.random.randint(0,num,size - data_size%size))\n",
    "    np.random.shuffle(index)\n",
    "    index  = index.reshape(-1,size)\n",
    "    return index\n",
    "\n",
    "def get_next_batch(x,y,index,size = 32,step = 0):\n",
    "    return x[index[step]],y[index[step]]\n",
    "\n",
    "\n",
    "\n",
    "                      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-53cfe08126ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_dict_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/Aro/Downloads/FlyAI/SpamMessage_FlyAI/data/input/words.dict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 训练数据的路径\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_dict' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "word_dict, word_dict_res = load_dict('./data/words.dict')\n",
    "vocab_size = max(word_dict.values()) + 1\n",
    "\n",
    "\n",
    "# 训练数据的路径\n",
    "DATA_PATH = os.path.join(sys.path[0], 'data', 'input')\n",
    "# 模型保存的路径\n",
    "MODEL_PATH = os.path.join(sys.path[0], 'data', 'output', 'model')\n",
    "# 训练log的输出路径\n",
    "LOG_PATH = os.path.join(sys.path[0], 'data', 'output', 'logs')\n",
    "\n",
    "\n",
    "# 超参\n",
    "embedding_dim = 64      # 嵌入层大小\n",
    "dnn_dim = 128           # Dense层大小\n",
    "max_seq_len = 128       # 最大句长\n",
    "num_filters = 64        # 卷积核数目\n",
    "kernel_size = 5         # 卷积核尺寸\n",
    "learning_rate = 1e-3    # 学习率\n",
    "numclass = 2            # 类别数\n",
    "\n",
    "tf.reset_default_graph()\n",
    "# 传值空间\n",
    "input_x = tf.placeholder(tf.int32, shape=[None, max_seq_len], name='input_x')\n",
    "input_y = tf.placeholder(tf.int32, shape=[None], name='input_y')\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "\n",
    "# define embedding layer\n",
    "# embedding 参照word embedding\n",
    "with tf.variable_scope('embedding'):\n",
    "    # 标准正态分布初始化\n",
    "    input_embedding = tf.Variable(\n",
    "        tf.truncated_normal(shape=[vocab_size, embedding_dim], stddev=0.1), name='encoder_embedding')\n",
    "\n",
    "with tf.name_scope(\"cnn\"):\n",
    "    # CNN layer\n",
    "    x_input_embedded = tf.nn.embedding_lookup(input_embedding, input_x)\n",
    "    conv = tf.layers.conv1d(x_input_embedded,num_filters,5,padding='same',name='conv')\n",
    "    # global max pooling layer\n",
    "    pooling = tf.reduce_max(conv, reduction_indices=[1])\n",
    "\n",
    "\n",
    "with tf.name_scope(\"score\"):\n",
    "    # 全连接层，后面接dropout以及relu激活\n",
    "    fc = tf.layers.dense(pooling, dnn_dim, name='fc')\n",
    "    fc = tf.contrib.layers.dropout(fc, keep_prob)\n",
    "    fc = tf.nn.relu(fc)\n",
    "    # 分类器\n",
    "    logits = tf.layers.dense(fc, numclass, name='fc1')\n",
    "    y_pred_cls = tf.argmax(tf.nn.softmax(logits), 1, name='y_pred')  # 预测类别\n",
    "\n",
    "with tf.name_scope(\"optimize\"):\n",
    "    # 将label进行onehot转化\n",
    "    one_hot_labels = tf.one_hot(input_y, depth=numclass, dtype=tf.float32)\n",
    "    # 损失函数，交叉熵\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=one_hot_labels)\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "    # 优化器\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    # 准确率\n",
    "    correct_pred = tf.equal(tf.argmax(one_hot_labels, 1), y_pred_cls)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='acc')\n",
    "\n",
    "with tf.name_scope(\"summary\"):\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "\n",
    "best_score = 0\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    train_writer = tf.summary.FileWriter(LOG_PATH, sess.graph)\n",
    "\n",
    "    # dataset.get_step() 获取数据的总迭代次数\n",
    "    total_step = int(np.ceil(len(X_train)/32) )\n",
    "    valid_step = int(np.ceil(len(X_test)/64) )\n",
    "    index = get_batch(X_train,size = 32)\n",
    "    valid_index = get_batch(X_test,size = 64)\n",
    "    for step in range(total_step):\n",
    "        x_train, y_train = get_next_batch(X_train,y_train,index,size = 32,step = step)\n",
    "        x_val, y_val = get_next_batch(X_test,y_test,index,size = 64,step = step%valid_step)\n",
    "\n",
    "        fetches = [loss, accuracy, train_op]\n",
    "        feed_dict = {input_x: x_train, input_y: y_train, keep_prob: 0.5}\n",
    "        loss_, accuracy_, _ = sess.run(fetches, feed_dict=feed_dict)\n",
    "\n",
    "        valid_acc = sess.run(accuracy, feed_dict={input_x: x_val, input_y: y_val, keep_prob: 1.0})\n",
    "        summary = sess.run(merged_summary, feed_dict=feed_dict)\n",
    "        train_writer.add_summary(summary, step)\n",
    "\n",
    "        cur_step = str(step + 1)\n",
    "        print('The Current step per total: {} | The Current loss: {} | The Current ACC: {} |'\n",
    "              ' The Current Valid ACC: {}'.format(cur_step, loss_, accuracy_, valid_acc))\n",
    "        if step % 100 == 0:  # 每隔100个step存储一次model文件\n",
    "            model.save_model(sess, MODEL_PATH, overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
